#!/bin/bash

set -o errexit

test_dir=$(realpath $(dirname $0))
. ${test_dir}/../functions
set_debug

deploy_cert_manager
create_infra $namespace

desc 'install PMM Server'
platform=kubernetes

helm uninstall monitoring || :
helm repo remove stable || :
helm repo add stable https://charts.helm.sh/stable
if [[ -n ${OPENSHIFT} ]]; then
	platform=openshift
	oc create sa pmm-server
	oc adm policy add-scc-to-user privileged -z pmm-server
	if [ -n "$OPERATOR_NS" ]; then
		timeout 30 oc delete clusterrolebinding $(kubectl get clusterrolebinding | grep 'pmm-psmdb-operator-' | awk '{print $1}') || :
		oc create clusterrolebinding pmm-psmdb-operator-cluster-wide --clusterrole=percona-server-mongodb-operator --serviceaccount=$namespace:pmm-server
		oc patch clusterrole/percona-server-mongodb-operator --type json -p='[{"op":"add","path": "/rules/-","value":{"apiGroups":["security.openshift.io"],"resources":["securitycontextconstraints"],"verbs":["use"],"resourceNames":["privileged"]}}]' ${OPERATOR_NS:+-n $OPERATOR_NS}
	else
		oc create rolebinding pmm-psmdb-operator-namespace-only --role percona-server-mongodb-operator --serviceaccount=$namespace:pmm-server
		oc patch role/percona-server-mongodb-operator --type json -p='[{"op":"add","path": "/rules/-","value":{"apiGroups":["security.openshift.io"],"resources":["securitycontextconstraints"],"verbs":["use"],"resourceNames":["privileged"]}}]'
	fi
	retry 10 60 helm install monitoring --set imageTag=$IMAGE_PMM_SERVER_TAG --set imageRepo=$IMAGE_PMM_SERVER_REPO --set platform=$platform --set sa=pmm-server --set supresshttp2=false https://percona-charts.storage.googleapis.com/pmm-server-${PMM_SERVER_VER}.tgz
else
	retry 10 60 helm install monitoring --set imageTag=$IMAGE_PMM_SERVER_TAG --set imageRepo=$IMAGE_PMM_SERVER_REPO --set platform=$platform https://percona-charts.storage.googleapis.com/pmm-server-${PMM_SERVER_VER}.tgz
fi
sleep 20
SERVICE="postgres"
until kubectl_bin exec monitoring-0 -- bash -c "pgrep -x $SERVICE >/dev/null"; do
	echo "Retry $retry"
	sleep 5
	let retry+=1
	if [ $retry -ge 20 ]; then
		echo "Max retry count $retry reached. Pmm-server can't start"
		exit 1
	fi
done

cluster="monitoring"

desc 'create secrets and start client'
kubectl_bin apply \
	-f $conf_dir/secrets.yml \
	-f $test_dir/conf/secrets.yml

yq ".spec.template.spec.volumes[0].secret.secretName=\"$cluster-ssl\"" \
	"$conf_dir/client_with_tls.yml" | kubectl_bin apply -f -
sleep 90

desc "create first PSMDB cluster $cluster"
apply_cluster "$test_dir/conf/$cluster-rs0.yml"
wait_for_running $cluster-rs0 3

desc 'check if pmm-client container is not enabled'
compare_kubectl statefulset/$cluster-rs0 "-no-pmm"
sleep 10

run_mongos \
	'db.createUser({user:"myApp",pwd:"myPass",roles:[{db:"myApp",role:"readWrite"}]})' \
	"userAdmin:userAdmin123456@$cluster-mongos.$namespace" "" "" \
	"--tlsCertificateKeyFile /tmp/tls.pem --tlsCAFile /etc/mongodb-ssl/ca.crt --tls"
run_mongos \
	'sh.enableSharding("myApp")' \
	"clusterAdmin:clusterAdmin123456@$cluster-mongos.$namespace" "" "" \
	"--tlsCertificateKeyFile /tmp/tls.pem --tlsCAFile /etc/mongodb-ssl/ca.crt --tls"
insert_data_mongos "100500" "myApp" \
	"--tlsCertificateKeyFile /tmp/tls.pem --tlsCAFile /etc/mongodb-ssl/ca.crt --tls"
insert_data_mongos "100600" "myApp" \
	"--tlsCertificateKeyFile /tmp/tls.pem --tlsCAFile /etc/mongodb-ssl/ca.crt --tls"
insert_data_mongos "100700" "myApp" \
	"--tlsCertificateKeyFile /tmp/tls.pem --tlsCAFile /etc/mongodb-ssl/ca.crt --tls"

desc 'add PMM_SERVER_API_KEY for secret some-users'
API_KEY=$(curl --insecure -X POST -H "Content-Type: application/json" -d '{"name":"operator", "role": "Admin"}' "https://admin:admin@"$(get_service_endpoint monitoring-service)"/graph/api/auth/keys" | jq .key)
kubectl_bin patch secret some-users --type merge --patch '{"stringData": {"PMM_SERVER_API_KEY": '$API_KEY'}}'

desc 'check if all 3 Pods started'
wait_for_running $cluster-rs0 3
# wait for prometheus
sleep 90

desc 'check if pmm-client container enabled'
compare_kubectl statefulset/$cluster-rs0
compare_kubectl service/$cluster-rs0
compare_kubectl service/$cluster-mongos
compare_kubectl statefulset/$cluster-cfg
compare_kubectl statefulset/$cluster-mongos

desc 'check mongod metrics'
get_metric_values node_boot_time_seconds $namespace-$cluster-rs0-1 admin:admin
get_metric_values mongodb_connections $namespace-$cluster-rs0-1 admin:admin

desc 'check mongo config  metrics'
get_metric_values node_boot_time_seconds $namespace-$cluster-cfg-1 admin:admin
get_metric_values mongodb_connections $namespace-$cluster-cfg-1 admin:admin

desc 'check mongos metrics'
MONGOS_POD_NAME=$(kubectl get pod -l app.kubernetes.io/component=mongos -o jsonpath="{.items[0].metadata.name}")
get_metric_values node_boot_time_seconds $namespace-$MONGOS_POD_NAME admin:admin
#get_metric_values mongodb_mongos_connections ${cluster%%-rs0}-mongos-0

# wait for QAN
sleep 90

desc 'check QAN data'
get_qan_values mongodb "dev-mongod" admin:admin
get_qan_values mongodb "dev-mongos" admin:admin

if [[ -n ${OPENSHIFT} ]]; then
	oc adm policy remove-scc-from-user privileged -z percona-server-mongodb-operator
fi

if [[ $(kubectl_bin logs monitoring-rs0-0 pmm-client | grep -c 'cannot auto discover databases and collections') != 0 ]]; then
	echo "error: cannot auto discover databases and collections"
	exit 1
fi

desc 'check for passwords leak'
check_passwords_leak

helm uninstall monitoring
destroy $namespace

desc 'test passed'
