#!/bin/bash

GIT_COMMIT=$(git rev-parse HEAD)
GIT_BRANCH=${VERSION:-$(git rev-parse --abbrev-ref HEAD | sed -e 's^/^-^g; s^[.]^-^g;' | sed -e 's/_/-/g' | tr '[:upper:]' '[:lower:]')}
API="psmdb.percona.com/v1"
OPERATOR_VERSION="1.14.0"
IMAGE=${IMAGE:-"perconalab/percona-server-mongodb-operator:${GIT_BRANCH}"}
IMAGE_PMM=${IMAGE_PMM:-"perconalab/pmm-client:dev-latest"}
IMAGE_MONGOD=${IMAGE_MONGOD:-"perconalab/percona-server-mongodb-operator:main-mongod4.4"}
IMAGE_BACKUP=${IMAGE_BACKUP:-"perconalab/percona-server-mongodb-operator:main-backup"}
SKIP_BACKUPS_TO_AWS_GCP_AZURE=${SKIP_BACKUPS_TO_AWS_GCP_AZURE:-1}
PMM_SERVER_VER=${PMM_SERVER_VER:-"9.9.9"}
IMAGE_PMM_SERVER_REPO=${IMAGE_PMM_SERVER_REPO:-"perconalab/pmm-server"}
IMAGE_PMM_SERVER_TAG=${IMAGE_PMM_SERVER_TAG:-"dev-latest"}
CERT_MANAGER_VER="1.8.0"
tmp_dir=$(mktemp -d)
sed=$(which gsed || which sed)
date=$(which gdate || which date)

test_name=$(basename $test_dir)
namespace="${test_name}-${RANDOM}"
replica_namespace="${test_name}-replica-${RANDOM}"
conf_dir=$(realpath $test_dir/../conf || :)
src_dir=$(realpath $test_dir/../..)
logs_dir=$(realpath $test_dir/../logs)

if [[ ${ENABLE_LOGGING} == "true" ]]; then
	if [ ! -d "${logs_dir}" ]; then
		mkdir "${logs_dir}"
	fi
	exec &> >(tee ${logs_dir}/${test_name}.log)
	echo "Log: ${logs_dir}/${test_name}.log"
fi

if [ -f "$conf_dir/cloud-secret.yml" ]; then
	SKIP_BACKUPS_TO_AWS_GCP_AZURE=''
fi

if oc get projects; then
	OPENSHIFT=4
fi

if [ $(kubectl version -o json | jq -r '.serverVersion.gitVersion' | grep "\-eks\-") ]; then
	EKS=1
else
	EKS=0
fi

KUBE_VERSION=$(kubectl version -o json | jq -r '.serverVersion.major + "." + .serverVersion.minor' | $sed -r 's/[^0-9.]+//g')

version_gt() {
	# return true if kubernetes version equal or greater than desired
	if [ $(echo "${KUBE_VERSION} >= $1" | bc -l) -eq 1 ]; then
		return 0
	else
		return 1
	fi
}

apply_s3_storage_secrets() {
	if [ -z "$SKIP_BACKUPS_TO_AWS_GCP_AZURE" ]; then
		kubectl_bin apply \
			-f $conf_dir/minio-secret.yml \
			-f $conf_dir/cloud-secret.yml
	else
		kubectl_bin apply \
			-f $conf_dir/minio-secret.yml
	fi
}

create_namespace() {
	local namespace="$1"
	local skip_clean_namespace="$2"

	if [[ ${CLEAN_NAMESPACE} == 1 ]] && [[ -z ${skip_clean_namespace} ]]; then
		kubectl_bin get ns \
			| egrep -v "^kube-|^default|Terminating|psmdb-operator|openshift|^NAME" \
			| awk '{print$1}' \
			| xargs kubectl delete ns &
	fi

	if [ -n "${OPENSHIFT}" ]; then
		if [ -n "$OPERATOR_NS" -a $(oc get project "$OPERATOR_NS" -o json | jq -r '.metadata.name') ]; then
			oc delete --grace-period=0 --force=true project "$namespace" && sleep 120 || :
		else
			oc delete project "$namespace" && sleep 40 || :
		fi
		oc new-project "$namespace"
		oc project "$namespace"
		oc adm policy add-scc-to-user hostaccess -z default || :
	else
		kubectl_bin delete namespace "$namespace" || :
		wait_for_delete "namespace/$namespace"
		kubectl_bin create namespace "$namespace"
		kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"
	fi
}

get_operator_pod() {
	kubectl_bin get pods \
		--selector=name=percona-server-mongodb-operator \
		-o 'jsonpath={.items[].metadata.name}' ${OPERATOR_NS:+-n $OPERATOR_NS}
}

wait_pod() {
	local pod=$1

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl_bin get pod/$pod -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin describe pod/$pod
			kubectl_bin logs $pod
			kubectl_bin logs ${OPERATOR_NS:+-n $OPERATOR_NS} $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_cron() {
	local backup=$1

	set +o xtrace
	retry=0
	echo -n $backup
	until kubectl_bin get cronjob/$backup -o jsonpath='{.status.lastScheduleTime}' 2>/dev/null | grep 'T'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs ${OPERATOR_NS:+-n $OPERATOR_NS} $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_backup_agent() {
	local agent_pod=$1

	set +o xtrace
	retry=0
	echo -n $agent_pod
	until [ "$(kubectl_bin logs $agent_pod -c backup-agent | egrep -v "\[ERROR\] pitr: check if on:|node:|starting PITR routine|\[agentCheckup\]" | cut -d' ' -f3- | tail -n 1)" == "listening for the commands" ]; do
		sleep 5
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs $agent_pod -c backup-agent \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	echo
	set -o xtrace
}

wait_backup() {
	local backup_name=$1

	set +o xtrace
	retry=0
	echo -n $backup_name
	until [ "$(kubectl_bin get psmdb-backup $backup_name -o jsonpath='{.status.state}')" == "ready" ]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs ${OPERATOR_NS:+-n $OPERATOR_NS} $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	echo
	set -o xtrace
}

run_restore() {
	local backup_name=$1
	let last_pod="$2-1" || :
	local isSharded=${3:-0}
	local cluster_pfx=$4
	if [ $isSharded -eq 1 ]; then
		run_mongos \
			'use myApp\n db.test.insert({ x: 100501 })' \
			"myApp:myPass@$cluster$cluster_pfx.$namespace"
		compare_mongos_cmd "find" "myApp:myPass@$cluster$cluster_pfx.$namespace" "-2nd"
	else
		run_mongo \
			'use myApp\n db.test.insert({ x: 100501 })' \
			"myApp:myPass@$cluster$cluster_pfx.$namespace"

		for i in $(seq 0 $last_pod); do
			compare_mongo_cmd "find" "myApp:myPass@$cluster$cluster_pfx-${i}.$cluster$cluster_pfx.$namespace" "-2nd"
		done
	fi
	cat $test_dir/conf/restore.yml \
		| $sed -e "s/name:/name: restore-$backup_name/" \
		| $sed -e "s/backupName:/backupName: $backup_name/" \
		| kubectl_bin apply -f -
}

run_restore_backupsource() {
	local backupName=$1
	local backupDest=$2
	local storageName=$3

	if [ -z "$storageName" ]; then
		cat $test_dir/conf/restore-backupsource.yml \
			| $sed -e "s/name:/name: restore-$backupName/" \
			| $sed -e "s/BACKUP-NAME/$backupDest/" \
			| $sed -e "/storageName/d" \
			| kubectl_bin apply -f -

		return
	fi

	cat $test_dir/conf/restore-backupsource.yml \
		| $sed -e "s/name:/name: restore-$backupName/" \
		| $sed -e "s/BACKUP-NAME/$backupDest/" \
		| $sed -e "s/storageName:/storageName: $storageName/" \
		| kubectl_bin apply -f -
}

wait_restore() {
	local isSharded=${3:-0}
	local cluster_pfx=$4

	wait_restore_object "${1}"
	echo
	set -o xtrace

	sleep 20
	simple_data_check "${cluster}" "${2}" "$isSharded" "$cluster_pfx"
}

wait_deployment() {
	local name=$1

	sleep 10
	retry=0
	echo -n $name
	until kubectl_bin get deployment $name ${OPERATOR_NS:+-n $OPERATOR_NS} >/dev/null \
        && [ "$(kubectl_bin get deployment $name -o jsonpath='{.status.replicas}' ${OPERATOR_NS:+-n $OPERATOR_NS})" == "$(kubectl_bin get deployment $name -o jsonpath='{.status.readyReplicas}' ${OPERATOR_NS:+-n $OPERATOR_NS})" ]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs ${OPERATOR_NS:+-n $OPERATOR_NS} $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
}

simple_data_check() {
	cluster_name=${1}
	let last_pod="$2-1" || :
	local isSharded=${3:-0}
	local cluster_pfx=$4

	if [ $isSharded -eq 1 ]; then
		sleep 10 # give time for start mongos
		wait_cluster_consistency "${cluster_name}"
		compare_mongos_cmd "find" "myApp:myPass@$cluster$cluster_pfx.$namespace"
	else
		for i in $(seq 0 $last_pod); do
			compare_mongo_cmd "find" "myApp:myPass@${cluster_name}${cluster_pfx}-${i}.${cluster_name}${cluster_pfx}.$namespace"
		done
	fi
}

wait_restore_object() {
	local backup_name=$1

	set +o xtrace
	retry=0
	echo -n $backup_name
	until [ "$(kubectl_bin get psmdb-restore restore-$backup_name -o jsonpath='{.status.state}')" == "ready" ]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs ${OPERATOR_NS:+-n $OPERATOR_NS} $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
}

apply_rbac() {
	local operator_namespace=${OPERATOR_NS:-'psmdb-operator'}
	local rbac=${1:-'rbac'}

	cat ${src_dir}/deploy/${rbac}.yaml \
		| sed -e "s^namespace: .*^namespace: $operator_namespace^" \
		| kubectl_bin apply ${OPERATOR_NS:+-n $OPERATOR_NS} -f -
}

deploy_operator() {
	desc 'start operator'

	local cr_file
	if [ -f "${test_dir}/conf/crd.yaml" ]; then
		cr_file="${test_dir}/conf/crd.yaml"
	else
		cr_file="${src_dir}/deploy/crd.yaml"
	fi

	kubectl_bin apply --server-side --force-conflicts -f "${cr_file}"
	if [ -n "$OPERATOR_NS" ]; then
		apply_rbac cw-rbac
		yq eval '
			(.spec.template.spec.containers[].image = "'${IMAGE}'") |
			((.. | select(.[] == "DISABLE_TELEMETRY")) |= .value="true") |
			((.. | select(.[] == "LOG_LEVEL")) |= .value="DEBUG")' ${src_dir}/deploy/cw-operator.yaml \
			| kubectl_bin apply -f -
	else
		apply_rbac rbac
		yq eval '
			(.spec.template.spec.containers[].image = "'${IMAGE}'") |
			((.. | select(.[] == "DISABLE_TELEMETRY")) |= .value="true") |
			((.. | select(.[] == "LOG_LEVEL")) |= .value="DEBUG")' ${src_dir}/deploy/operator.yaml \
			| kubectl_bin apply -f -
	fi
	sleep 2
	wait_pod $(get_operator_pod)
}

function deploy_operator_gh() {
	local git_tag="$1"

	desc 'start operator'
	kubectl_bin apply -f "https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/${git_tag}/deploy/crd.yaml"
	local rbac_yaml="rbac"
	local operator_yaml="operator"
	if [ -n "${OPERATOR_NS}" ]; then
		rbac_yaml="cw-rbac"
		operator_yaml="cw-operator"
	fi
	kubectl_bin apply -f "https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/${git_tag}/deploy/${rbac_yaml}.yaml"
	curl -s "https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/${git_tag}/deploy/${operator_yaml}.yaml" >"${tmp_dir}/${operator_yaml}_${git_tag}.yaml"

	$sed -i -e "s^image: .*^image: ${IMAGE}^" "${tmp_dir}/${operator_yaml}_${git_tag}.yaml"
	kubectl_bin apply -f "${tmp_dir}/${operator_yaml}_${git_tag}.yaml"

	sleep 2
	wait_pod "$(get_operator_pod)"
}

deploy_minio() {
	desc 'install Minio'
	helm uninstall minio-service || :
	helm repo remove minio || :
	helm repo add minio https://helm.min.io/
	# kubectl_bin delete pvc minio-service --force
	retry 10 60 helm install minio-service \
		--version 8.0.5 \
		--set accessKey=some-access-key \
		--set secretKey=some-secret-key \
		--set service.type=ClusterIP \
		--set configPathmc=/tmp/.minio/ \
		--set persistence.size=2G \
		--set environment.MINIO_REGION=us-east-1 \
		--set environment.MINIO_HTTP_TRACE=/tmp/trace.log \
		--set securityContext.enabled=false \
		minio/minio
	MINIO_POD=$(kubectl_bin get pods --selector=release=minio-service -o 'jsonpath={.items[].metadata.name}')
	wait_pod $MINIO_POD

	if [ -n "$OPERATOR_NS" ]; then
		kubectl_bin create svc -n ${OPERATOR_NS} externalname minio-service --external-name="minio-service.${namespace}.svc.cluster.local" --tcp="9000"
	fi

	# create bucket
	kubectl_bin run -i --rm aws-cli --image=perconalab/awscli --restart=Never -- \
		bash -c 'AWS_ACCESS_KEY_ID=some-access-key AWS_SECRET_ACCESS_KEY=some-secret-key AWS_DEFAULT_REGION=us-east-1 \
        /usr/bin/aws --endpoint-url http://minio-service:9000 s3 mb s3://operator-testing'
}

deploy_vault() {
	desc 'install Vault'
	helm uninstall vault-service || :
	helm repo remove hashicorp || :
	helm repo add hashicorp https://helm.releases.hashicorp.com

	destroy_vault

	retry 10 60 helm install vault-service hashicorp/vault

	until kubectl_bin get pod/vault-service-0 -o jsonpath='{.status.phase}' 2>/dev/null | grep 'Running'; do
		sleep 1
	done

	kubectl_bin exec pod/vault-service-0 -- vault operator init -key-shares=1 -key-threshold=1 -format=json >"${tmp_dir}"/vault-init
	local unsealKey=$(jq -r ".unseal_keys_b64[]" <"${tmp_dir}"/vault-init)
	local token=$(jq -r ".root_token" <"${tmp_dir}"/vault-init)
	kubectl_bin exec pod/vault-service-0 -- vault operator unseal "${unsealKey}"
	kubectl_bin exec -it pod/vault-service-0 -- sh <<EOF
vault login "${token}"
vault secrets enable -path secret kv-v2
EOF
	kubectl_bin create secret generic vault-secret --from-literal=token="${token}"
}

destroy_vault() {
	local vault_ns=$(helm list --all-namespaces --filter vault-service | tail -n1 | awk -F' ' '{print $2}' | sed 's/NAMESPACE//')

	desc 'destroy vault'
	for i in $(kubectl api-resources | grep vault | awk '{print $1}'); do timeout 30 kubectl delete ${i} --all --all-namespaces || :; done
	if [ -n "${vault_ns}" ]; then
		helm uninstall vault-service --namespace ${vault_ns} || :
	fi
	timeout 30 kubectl delete crd $(kubectl get crd | grep 'vault' | awk '{print $1}') || :
	timeout 30 kubectl delete clusterrolebinding $(kubectl get clusterrolebinding | grep 'vault' | awk '{print $1}') || :
	timeout 30 kubectl delete clusterrole $(kubectl get clusterrole | grep 'vault' | awk '{print $1}') || :
	timeout 30 kubectl delete mutatingwebhookconfiguration $(kubectl get mutatingwebhookconfiguration | grep 'vault' | awk '{print $1}') || :
}

deploy_chaos_mesh() {
	local chaos_mesh_ns=$1

	destroy_chaos_mesh

	desc 'install chaos-mesh'
	helm repo add chaos-mesh https://charts.chaos-mesh.org
	if version_gt "1.19"; then
		helm install chaos-mesh chaos-mesh/chaos-mesh --namespace=${chaos_mesh_ns} --set chaosDaemon.runtime=containerd --set chaosDaemon.socketPath=/run/containerd/containerd.sock --set dashboard.create=false --version 2.1.5 --set clusterScoped=false --set controllerManager.targetNamespace=${chaos_mesh_ns}
	else
		helm install chaos-mesh chaos-mesh/chaos-mesh --namespace=${chaos_mesh_ns} --set dashboard.create=false --version 2.1.5 --set clusterScoped=false --set controllerManager.targetNamespace=${chaos_mesh_ns}
	fi
	sleep 10
}

destroy_chaos_mesh() {
	local chaos_mesh_ns=$(helm list --all-namespaces --filter chaos-mesh | tail -n1 | awk -F' ' '{print $2}' | sed 's/NAMESPACE//')

	desc 'destroy chaos-mesh'
	for i in $(kubectl api-resources | grep chaos-mesh | awk '{print $1}'); do timeout 30 kubectl delete ${i} --all --all-namespaces || :; done
	if [ -n "${chaos_mesh_ns}" ]; then
		helm uninstall chaos-mesh --namespace ${chaos_mesh_ns} || :
	fi
	timeout 30 kubectl delete crd $(kubectl get crd | grep 'chaos-mesh.org' | awk '{print $1}') || :
	timeout 30 kubectl delete clusterrolebinding $(kubectl get clusterrolebinding | grep 'chaos-mesh' | awk '{print $1}') || :
	timeout 30 kubectl delete clusterrole $(kubectl get clusterrole | grep 'chaos-mesh' | awk '{print $1}') || :
}

retry() {
	local max=$1
	local delay=$2
	shift 2 # cut delay and max args
	local n=1

	until "$@"; do
		if [[ $n -ge $max ]]; then
			echo "The command '$@' has failed after $n attempts."
			exit 1
		fi
		((n++))
		sleep $delay
	done
}

wait_for_running() {
	local name="$1"
	let last_pod="$(($2 - 1))" || :
	local check_cluster_readyness="${3:-true}"

	local rs_name=${name/*-/}
	local cluster_name=${name/-$rs_name/}
	for i in $(seq 0 $last_pod); do
		if [[ $i -eq $last_pod && $(kubectl_bin get psmdb ${cluster_name} -o jsonpath='{.spec.replsets[?(@.name=="'${rs_name}'")].arbiter.enabled}') == "true" ]]; then
			wait_pod ${name}-arbiter-0
		else
			wait_pod ${name}-${i}
		fi
	done
	if [[ $(kubectl_bin get psmdb ${cluster_name} -o jsonpath='{.spec.replsets[?(@.name=="'${rs_name}'")].non_voting.enabled}') == "true" ]]; then
		last_pod=$(($(kubectl_bin get psmdb ${cluster_name} -o jsonpath='{.spec.replsets[?(@.name=="'${rs_name}'")].non_voting.size}') - 1))
		for i in $(seq 0 $last_pod); do
			wait_pod ${name}-nv-${i}
		done
	fi
	sleep 10
	if [[ ${check_cluster_readyness} == "true" ]]; then
		set +x
		echo -n "Waiting for cluster readyness"
		local timeout=0
		until [[ $(kubectl_bin get psmdb ${cluster_name} -o jsonpath={.status.state}) == "ready" ]]; do
			sleep 1
			timeout=$((timeout + 1))
			echo -n '.'
			if [[ ${timeout} -gt 1500 ]]; then
				echo
				echo "Waiting timeout has been reached. Exiting..."
				exit 1
			fi
		done
		echo
		set -x
	fi
}

wait_for_delete() {
	local res="$1"

	set +o xtrace
	echo -n "$res - "
	retry=0
	until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 60 ]; then
			kubectl_bin logs ${OPERATOR_NS:+-n $OPERATOR_NS} $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

function compare_generation() {
	local generation="$1"
	local resource_type="$2"
	local resource_name="$3"
	local current_generation

	current_generation="$(kubectl_bin get ${resource_type} "${resource_name}" -o jsonpath='{.metadata.generation}')"
	if [[ ${generation} != "${current_generation}" ]]; then
		echo "Generation for ${resource_type}/${resource_name} is: ${current_generation}, but should be: ${generation}"
		exit 1
	fi
}

compare_kubectl() {
	local resource="$1"
	local postfix="$2"
	local expected_result=${test_dir}/compare/${resource//\//_}${postfix}.yml
	local new_result="${tmp_dir}/${resource//\//_}.yml"

	if [ -n "$OPENSHIFT" -a -f ${expected_result//.yml/-oc.yml} ]; then
		expected_result=${expected_result//.yml/-oc.yml}
		if [ "$OPENSHIFT" = 4 -a -f ${expected_result//-oc.yml/-4-oc.yml} ]; then
			expected_result=${expected_result//-oc.yml/-4-oc.yml}
		fi
	fi

	kubectl_bin get -o yaml ${resource} \
		| yq eval '
			del(.metadata.ownerReferences[].apiVersion) |
			del(.metadata.managedFields) |
			del(.. | select(has("creationTimestamp")).creationTimestamp) |
			del(.. | select(has("namespace")).namespace) |
			del(.. | select(has("uid")).uid) |
			del(.metadata.resourceVersion) |
			del(.metadata.selfLink) |
			del(.. | select(has("image")).image) |
			del(.. | select(has("clusterIP")).clusterIP) |
			del(.. | select(has("clusterIPs")).clusterIPs) |
			del(.. | select(has("dataSource")).dataSource) |
			del(.. | select(has("procMount")).procMount) |
			del(.. | select(has("storageClassName")).storageClassName) |
			del(.. | select(has("finalizers")).finalizers) |
			del(.. | select(has("kubernetes.io/pvc-protection"))."kubernetes.io/pvc-protection") |
			del(.. | select(has("volumeName")).volumeName) |
			del(.. | select(has("volume.beta.kubernetes.io/storage-provisioner"))."volume.beta.kubernetes.io/storage-provisioner") |
			del(.. | select(has("volume.kubernetes.io/storage-provisioner"))."volume.kubernetes.io/storage-provisioner") |
			del(.spec.volumeMode) |
			del(.. | select(has("volume.kubernetes.io/selected-node"))."volume.kubernetes.io/selected-node") |
			del(.. | select(has("percona.com/ssl-hash"))."percona.com/ssl-hash") |
			del(.. | select(has("percona.com/ssl-internal-hash"))."percona.com/ssl-internal-hash") |
			del(.spec.volumeClaimTemplates[].spec.volumeMode | select(. == "Filesystem")) |
			del(.. | select(has("healthCheckNodePort")).healthCheckNodePort) |
			del(.. | select(has("nodePort")).nodePort) |
			del(.status) |
			(.. | select(tag == "!!str")) |= sub("'$namespace'", "NAME_SPACE") |
			del(.spec.volumeClaimTemplates[].apiVersion) |
			del(.spec.volumeClaimTemplates[].kind) |
			del(.spec.ipFamilies) |
			del(.spec.ipFamilyPolicy) |
			(.. | select(. == "extensions/v1beta1")) = "apps/v1" |
			(.. | select(. == "batch/v1beta1")) = "batch/v1" ' - >${new_result}

	diff -u "$expected_result" "$new_result"
}

run_mongo() {
	local command="$1"
	local uri="$2"
	local driver=${3:-mongodb+srv}
	local suffix=${4:-.svc.cluster.local}
	local client_container=$(kubectl_bin get pods --selector=name=psmdb-client -o 'jsonpath={.items[].metadata.name}')
	local mongo_flag="$5"
	kubectl_bin exec ${client_container} -- \
		bash -c "printf '$command\n' | mongo $driver://$uri$suffix/admin?ssl=false\&replicaSet=rs0 $mongo_flag"

}

run_mongos() {
	local command="$1"
	local uri="$2"
	local driver=${3:-mongodb}
	local suffix=${4:-.svc.cluster.local}
	local client_container=$(kubectl_bin get pods --selector=name=psmdb-client -o 'jsonpath={.items[].metadata.name}')
	local mongo_flag="$5"

	kubectl_bin exec ${client_container} -- \
		bash -c "printf '$command\n' | mongo $driver://$uri$suffix/admin $mongo_flag"
}

run_script_mongos() {
	local script="$1"
	local uri="$2"
	local driver=${3:-mongodb}
	local suffix=${4:-.svc.cluster.local}
	local client_container=$(kubectl_bin get pods --selector=name=psmdb-client -o 'jsonpath={.items[].metadata.name}')
	local mongo_flag="$5"

	name="$(basename $script)"
	kubectl_bin cp ${script} $namespace/${client_container}:/tmp
	kubectl_bin exec ${client_container} -- \
		bash -c "mongo $driver://$uri$suffix/admin $mongo_flag /tmp/${name}"
}

get_service_ip() {
	local service=$1
	local server_type=${2:-rs0}
	if [ "$(kubectl_bin get psmdb/${service/-$server_type*/} -o 'jsonpath={.spec.replsets[].expose.enabled}')" != "true" ]; then
		echo -n $service.${service/-rs0*/}-rs0
		return
	fi
	while (kubectl_bin get service/$service -o 'jsonpath={.spec.type}' 2>&1 || :) | grep -q NotFound; do
		sleep 1
	done
	if [ "$(kubectl_bin get service/$service -o 'jsonpath={.spec.type}')" = "ClusterIP" ]; then
		kubectl_bin get service/$service -o 'jsonpath={.spec.clusterIP}'
		return
	fi
	until kubectl_bin get service/$service -o 'jsonpath={.status.loadBalancer.ingress[]}' 2>&1 | egrep -q "hostname|ip"; do
		sleep 1
	done
	kubectl_bin get service/$service -o 'jsonpath={.status.loadBalancer.ingress[].ip}'
	kubectl_bin get service/$service -o 'jsonpath={.status.loadBalancer.ingress[].hostname}'
}

compare_mongo_cmd() {
	local command="$1"
	local uri="$2"
	local postfix="$3"
	local suffix="$4"

	run_mongo "use myApp\n db.test.${command}()" "$uri" "mongodb" "$suffix" \
		| egrep -v 'I NETWORK|W NETWORK|F NETWORK|Error saving history file|Percona Server for MongoDB|connecting to:|Unable to reach primary for set|Implicit session:|versions do not match' \
		| $sed -re 's/ObjectId\("[0-9a-f]+"\)//; s/-[0-9]+.svc/-xxx.svc/' \
			>$tmp_dir/${command}
	diff ${test_dir}/compare/${command}${postfix}.json $tmp_dir/${command}
}

compare_mongos_cmd() {
	local command="$1"
	local uri="$2"
	local postfix="$3"
	local suffix="$4"

	run_mongos "use myApp\n db.test.${command}()" "$uri" "mongodb" "$suffix" \
		| egrep -v 'I NETWORK|W NETWORK|Error saving history file|Percona Server for MongoDB|connecting to:|Unable to reach primary for set|Implicit session:|versions do not match' \
		| $sed -re 's/ObjectId\("[0-9a-f]+"\)//; s/-[0-9]+.svc/-xxx.svc/' \
			>$tmp_dir/${command}
	diff ${test_dir}/compare/${command}${postfix}.json $tmp_dir/${command}
}

get_mongo_primary_endpoint() {
	local uri="$1"

	run_mongo 'db.isMaster().me' "$uri" "mongodb" ":27017" \
		| egrep -v "Time|Percona Server for MongoDB|bye|BinData|NumberLong|connecting to|Error saving history file|I NETWORK|W NETWORK|Implicit session:|versions do not match" \
		| sed -e 's^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]T[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\.[0-9][0-9][0-9]+[0-9][0-9][0-9][0-9]^^' \
		| grep ":27017$"
}

get_mongo_primary() {
	local uri="$1"
	local cluster="$2"

	endpoint=$(get_mongo_primary_endpoint $uri)
	if [[ $endpoint =~ ".$cluster" ]]; then
		echo $endpoint \
			| cut -d . -f 1
	else
		kubectl_bin get service -o wide \
			| grep " ${endpoint/:*/} " \
			| awk '{print$1}'
	fi
}

compare_mongo_user() {
	local uri="$1"
	local user=$(echo $uri | cut -d : -f 1)
	local expected_result=${test_dir}/compare/$user.json

	if [[ $IMAGE_MONGOD =~ 4\.0 ]] && [ -f ${test_dir}/compare/$user-40.json ]; then
		expected_result=${test_dir}/compare/$user-40.json
	fi
	if [[ $IMAGE_MONGOD =~ 4\.2 ]] && [ -f ${test_dir}/compare/$user-42.json ]; then
		expected_result=${test_dir}/compare/$user-42.json
	fi
	if [[ $IMAGE_MONGOD =~ 4\.4 ]] && [ -f ${test_dir}/compare/$user-44.json ]; then
		expected_result=${test_dir}/compare/$user-44.json
	fi
	if [[ $IMAGE_MONGOD =~ 5\.0 ]] && [ -f ${test_dir}/compare/$user-50.json ]; then
		expected_result=${test_dir}/compare/$user-50.json
	fi

	run_mongo 'db.runCommand({connectionStatus:1,showPrivileges:true})' "$uri" \
		| egrep -v "Time|Percona Server for MongoDB|bye|BinData|NumberLong|connecting to|Error saving history file|I NETWORK|W NETWORK|Implicit session:|versions do not match" \
		| sed -e 's^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]T[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\.[0-9][0-9][0-9]+[0-9][0-9][0-9][0-9]^^' \
		| $sed -e '/"ok" : 1/,+4d' \
		| sed -e '$s/,$/}/' \
		| jq '.authInfo.authenticatedUserPrivileges|=sort_by(.resource.anyResource, .resource.cluster, .resource.db, .resource.collection)|.authInfo.authenticatedUserRoles|=sort_by(.role)' \
			>$tmp_dir/$user.json
	diff -u $expected_result $tmp_dir/$user.json
}

start_gke() {
	gcloud container clusters create operator-testing-$RANDOM --zone europe-west3-c --project cloud-dev-112233 --preemptible --cluster-version 1.11
}

get_pumba() {
	kubectl_bin get pods \
		--selector=name=pumba \
		-o 'jsonpath={.items[].metadata.name}'
}

run_pumba() {
	local cmd="$*"
	kubectl_bin exec -it "$(get_pumba)" -- /pumba -l info ${cmd}
}

deploy_cert_manager() {
	kubectl_bin create namespace cert-manager || :
	kubectl_bin label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
	kubectl_bin apply -f "https://github.com/jetstack/cert-manager/releases/download/v${CERT_MANAGER_VER}/cert-manager.yaml" --validate=false || : 2>/dev/null
	sleep 30
}

delete_crd() {
	kubectl_bin delete psmdb-backup --all || :
	kubectl_bin delete psmdb-restore --all || :
	kubectl_bin delete psmdb --all || :

	kubectl_bin get crd/perconaservermongodbs.psmdb.percona.com \
		&& timeout 120 kubectl delete -f "${src_dir}/deploy/crd.yaml" || true

	kubectl_bin get crd/perconaservermongodbs.psmdb.percona.com \
		&& timeout 120 kubectl delete -f "${src_dir}/deploy/crd.yaml"

	local rbac_yaml='rbac.yaml'
	if [ -n "${OPERATOR_NS}" ]; then
		rbac_yaml='cw-rbac.yaml'
	fi
	kubectl_bin delete -f "${src_dir}/deploy/$rbac_yaml" || true
}

destroy() {
	local namespace="$1"
	local ignore_logs="${2:-false}"

	if [[ ${ignore_logs} == "false" ]]; then
		kubectl_bin logs ${OPERATOR_NS:+-n $OPERATOR_NS} $(get_operator_pod) \
			| grep -v 'level=info' \
			| grep -v 'level=debug' \
			| grep -v 'Getting tasks for pod' \
			| grep -v 'Getting pods from source' \
			| grep -v 'the object has been modified' \
			| grep -v 'get backup status: Job.batch' \
			| $sed -r 's/"ts":[0-9.]+//; s^limits-[0-9.]+/^^g' \
			| sort -u \
			| tee $tmp_dir/operator.log
	fi
	#TODO: maybe will be enabled later
	#diff $test_dir/compare/operator.log $tmp_dir/operator.log

	delete_crd

	kubectl_bin delete -f "https://github.com/jetstack/cert-manager/releases/download/v${CERT_MANAGER_VER}/cert-manager.yaml" 2>/dev/null || :
	if [ -n "$OPENSHIFT" ]; then
		oc delete --grace-period=0 --force=true project "$namespace" &
		if [ -n "$OPERATOR_NS" ]; then
			oc delete --grace-period=0 --force=true project "$OPERATOR_NS" &
		fi
	else
		kubectl_bin delete --grace-period=0 --force=true namespace "$namespace" &
		if [ -n "$OPERATOR_NS" ]; then
			kubectl_bin delete --grace-period=0 --force=true namespace "$OPERATOR_NS" &
		fi
	fi
	rm -rf ${tmp_dir}
}

desc() {
	set +o xtrace
	local msg="$@"
	printf "\n\n-----------------------------------------------------------------------------------\n"
	printf "$msg"
	printf "\n-----------------------------------------------------------------------------------\n\n"
	set -o xtrace
}

get_backup_dest() {
	local backup_name=$1

	kubectl_bin get psmdb-backup $backup_name -o jsonpath='{.status.destination}' \
		| sed -e 's/.json$//'
}

get_service_endpoint() {
	local service=$1

	local hostname=$(
		kubectl_bin get service/$service -o json \
			| jq '.status.loadBalancer.ingress[].hostname' \
			| sed -e 's/^"//; s/"$//;'
	)
	if [ -n "$hostname" -a "$hostname" != "null" ]; then
		echo $hostname
		return
	fi

	local ip=$(
		kubectl_bin get service/$service -o json \
			| jq '.status.loadBalancer.ingress[].ip' \
			| sed -e 's/^"//; s/"$//;'
	)
	if [ -n "$ip" -a "$ip" != "null" ]; then
		echo $ip
		return
	fi

	exit 1
}

get_metric_values() {
	local metric=$1
	local instance=$2
	local user_pass=$3
	local start=$($date -u "+%s" -d "-1 minute")
	local end=$($date -u "+%s")
	local endpoint=$(get_service_endpoint monitoring-service)

	curl -s -k "https://${user_pass}@$endpoint/graph/api/datasources/proxy/1/api/v1/query_range?query=min%28$metric%7Bnode_name%3D%7E%22$instance%22%7d%20or%20$metric%7Bnode_name%3D%7E%22$instance%22%7D%29&start=$start&end=$end&step=60" \
		| jq '.data.result[0].values[][1]' \
		| grep '^"[0-9]'

}

get_qan_values() {
	local instance=$1
	local start=$($date -u "+%Y-%m-%dT%H:%M:%S" -d "-30 minute")
	local end=$($date -u "+%Y-%m-%dT%H:%M:%S")
	local endpoint=$(get_service_endpoint monitoring-service)

	local uuid=$(
		curl -s -k "https://$endpoint/qan-api/instances?deleted=no" \
			| jq '.[] | select(.Subsystem == "mongo" and .Name == "'$instance'") | .UUID' \
			| sed -e 's/^"//; s/"$//;'
	)

	curl -s -k "https://$endpoint/qan-api/qan/profile/$uuid?begin=$start&end=$end&offset=0" \
		| jq '.Query[].Fingerprint'
}

get_qan20_values() {
	local instance=$1
	local user_pass=$2
	local start=$($date -u "+%Y-%m-%dT%H:%M:%S" -d "-30 minute")
	local end=$($date -u "+%Y-%m-%dT%H:%M:%S")
	local endpoint=$(get_service_endpoint monitoring-service)

	cat >payload.json <<EOF
{
   "columns":[
      "load",
      "num_queries",
      "query_time"
   ],
   "first_seen": false,
   "group_by": "queryid",
   "include_only_fields": [],
   "keyword": "",
   "labels": [
       {
           "key": "cluster",
           "value": ["monitoring"]
   }],
   "limit": 10,
   "offset": 0,
   "order_by": "-load",
   "main_metric": "load",
   "period_start_from": "$($date -u -d '-12 hour' '+%Y-%m-%dT%H:%M:%S%:z')",
   "period_start_to": "$($date -u '+%Y-%m-%dT%H:%M:%S%:z')"
}
EOF

	curl -s -k -XPOST -d @payload.json "https://${user_pass}@$endpoint/v0/qan/GetReport" \
		| jq '.rows[].fingerprint'
	rm -f payload.json
}

cat_config() {
	cat "$1" \
		| $sed -e "s#image:\$#image: $IMAGE_MONGOD#" \
		| $sed -e "s#image:.*-pmm\$#image: $IMAGE_PMM#" \
		| $sed -e "s#image:.*-backup\$#image: $IMAGE_BACKUP#" \
		| $sed -e "s#image: .*-mongod[34].*#image: $IMAGE_MONGOD#" \
		| $sed -e "s#image: .*-mongodb:[34].*#image: $IMAGE_MONGOD#" \
		| $sed -e "s#apply:.*#apply: Never#"
}

apply_cluster() {
	if [ -z "$SKIP_BACKUPS_TO_AWS_GCP_AZURE" ]; then
		cat_config "$1" \
			| kubectl_bin apply -f -
	else
		cat_config "$1" \
			| yq eval '
				del(.spec.backup.tasks.[1]) |
				del(.spec.backup.tasks.[1]) |
				del(.spec.backup.tasks.[1])' - \
			| kubectl_bin apply -f -
	fi
}

spinup_psmdb() {
	local cluster=$1
	local config=$2
	local size="${3:-3}"

	desc 'create first PSMDB cluster'
	apply_cluster $config

	desc 'check if Pod is started'
	wait_for_running "${cluster}" "$size"
	sleep 20

	compare_kubectl "statefulset/${cluster}"

	desc 'write data'

	run_mongo 'db.createUser({user: "myApp", pwd: "myPass", roles: [{ db: "myApp", role: "readWrite" }]})' \
		"userAdmin:userAdmin123456@${cluster}.${namespace}"

	run_mongo 'use myApp\n db.test.insert({ x: 100500 })' "myApp:myPass@${cluster}.${namespace}"
}

kubectl_bin() {
	local LAST_OUT="$(mktemp)"
	local LAST_ERR="$(mktemp)"
	local exit_status=0
	local timeout=4
	for i in $(seq 0 2); do
		set +e
		kubectl "$@" 1>"$LAST_OUT" 2>"$LAST_ERR"
		exit_status=$?
		set -e
		if [[ ${exit_status} != 0 ]]; then
			cat "$LAST_OUT"
			cat "$LAST_ERR" >&2
			sleep "$((timeout * i))"
		else
			break
		fi
	done
	cat "$LAST_OUT"
	cat "$LAST_ERR" >&2
	rm "$LAST_OUT" "$LAST_ERR"
	return ${exit_status}
}

patch_secret() {
	local secret=$1
	local key=$2
	local value=$3

	kubectl patch secret $secret -p="{\"data\":{\"$key\": \"$value\"}}"
}

getSecretData() {
	local secretName=$1
	local dataKey=$2
	local data=$(kubectl get secrets/${secretName} --template={{.data.${dataKey}}} | base64 -d)
	echo "$data"
}

check_mongo_auth() {
	local uri="$1"

	ping=$(run_mongo "db.runCommand({ ping: 1 }).ok" "$uri" "mongodb" "" "--quiet" | egrep -v 'I NETWORK|W NETWORK|Error saving history file|Percona Server for MongoDB|connecting to:|Unable to reach primary for set|Implicit session:|versions do not match')
	desc "ping return"
	if [ "${ping}" != "1" ]; then
		return 1
	fi
}

wait_cluster_consistency() {
	cluster_name=$1
	retry=0
	sleep 7 # wait for two reconcile loops ;)  3 sec x 2 times + 1 sec = 7 seconds
	until [[ "$(kubectl_bin get psmdb "${cluster_name}" -o jsonpath='{.status.state}')" == "ready" ]]; do
		let retry+=1
		if [ $retry -ge 32 ]; then
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
		echo 'waiting for cluster readyness'
		sleep 10
	done
}

run_backup() {
	local storage=$1

	kubectl_bin apply -f $test_dir/conf/backup-$storage.yml
}

check_backup_deletion() {
	path=$1
	storage_name=$2
	retry=0
	until [[ $(curl -sw '%{http_code}' -o /dev/null $path) -eq 403 ]] || [[ $(curl -sw '%{http_code}' -o /dev/null $path) -eq 404 ]]; do
		if [ $retry -ge 10 ]; then
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			echo "Backup was not removed from bucket -- $storage_name"
			exit 1
		fi
		echo "waiting for backup deletion $storage_name"
		sleep $((10 * 2 ** retry))
		((retry += 1))
	done
}

create_infra() {
	local ns="$1"

	delete_crd
	if [ -n "$OPERATOR_NS" ]; then
		create_namespace $OPERATOR_NS
		deploy_operator
		create_namespace $ns
	else
		create_namespace $ns
		deploy_operator
	fi
}

function create_infra_gh() {
	local ns="$1"
	local git_tag="$2"

	if [ -n "${OPERATOR_NS}" ]; then
		create_namespace "${OPERATOR_NS}"
		deploy_operator_gh "${git_tag}"
		create_namespace "${ns}"
	else
		create_namespace "${ns}"
		deploy_operator_gh "${git_tag}"
	fi
}
