#!/bin/bash

set -o errexit

test_dir=$(realpath "$(dirname "$0")")
. "${test_dir}/../functions"
set_debug

run_backup() {
	local storage=$1
	local backup_name=$2
	local base=${3:-true}

	local backup_type="incremental"

	if [[ $base == "true" ]]; then
		backup_type="incremental-base"
	fi

	yq "$test_dir/conf/backup.yml" \
		| $sed -e "s/name:/name: ${backup_name}/" \
		| $sed -e "s/storageName:/storageName: ${storage}/" \
		| yq ".spec.type=\"${backup_type}\"" \
		| kubectl_bin apply -f -
}

run_restore() {
	local backup_name=$1

	log "drop collection"
	run_mongos 'use myApp\n db.test.drop()' "myApp:myPass@${cluster}-mongos.${namespace}"
	log "check backup and restore -- $backup_name"

	yq "$test_dir/conf/restore.yml" \
		| $sed -e "s/name:/name: restore-${backup_name}/" \
		| $sed -e "s/backupName:/backupName: ${backup_name}/" \
		| kubectl_bin apply -f -
}

run_recovery_check() {
	local backup_name=$1
	local compare_suffix=${2:-"_restore"}
	local base=${3:-true}

	wait_restore "${backup_name}" "${cluster}" "requested" "0" "3000"
	echo

	compare_kubectl "statefulset/${cluster}-rs0" "${compare_suffix}"

	# we don't wait for cluster readiness here because the annotation gets removed then
	wait_restore "${backup_name}" "${cluster}" "ready" "0" "3000"
	kubectl_bin get psmdb ${cluster} -o yaml
	if [ $(kubectl_bin get psmdb ${cluster} -o yaml | yq '.metadata.annotations."percona.com/resync-pbm"') == null ]; then
		log "psmdb/${cluster} should be annotated with percona.com/resync-pbm after a incremental restore"
		exit 1
	fi
	echo

	wait_cluster_consistency ${cluster} 60
	wait_for_pbm_operations ${cluster}

	if [[ $base == true ]]; then
		compare_mongos_cmd "find" "myApp:myPass@${cluster}-mongos.${namespace}" "-sharded"
	else
		compare_mongos_cmd "find" "myApp:myPass@${cluster}-mongos.${namespace}" "-not-base-sharded"
	fi
}

check_exported_mongos_service_endpoint() {
	local host=$1

	if [ "$host" != "$(kubectl_bin get psmdb $cluster -o=jsonpath='{.status.host}')" ]; then
		log "Exported host is not correct after the restore"
		exit 1
	fi
}

create_infra "${namespace}"

deploy_minio
apply_s3_storage_secrets

### Case 1: Backup and restore on sharded cluster
desc 'Testing on sharded cluster'

log "Creating PSMDB cluster"
cluster="some-name"
kubectl_bin apply -f "${conf_dir}/secrets.yml"
apply_cluster "${test_dir}/conf/${cluster}-sharded.yml"
kubectl_bin apply -f "${conf_dir}/client_with_tls.yml"

log "check if all pods started"
wait_for_running ${cluster}-rs0 3
wait_for_running ${cluster}-cfg 3
wait_for_running ${cluster}-mongos 3
wait_cluster_consistency ${cluster}

lbEndpoint=$(kubectl_bin get svc $cluster-mongos -o=jsonpath='{.status}' \
	| jq -r 'select(.loadBalancer != null and .loadBalancer.ingress != null and .loadBalancer.ingress != []) | .loadBalancer.ingress[0] | if .ip then .ip else .hostname end')
if [ -z "$lbEndpoint" ]; then
	log "mongos service not exported correctly"
	exit 1
fi

run_mongos \
	'db.createUser({user:"myApp",pwd:"myPass",roles:[{db:"myApp",role:"readWrite"}]})' \
	"userAdmin:userAdmin123456@${cluster}-mongos.${namespace}"
sleep 1
run_mongos \
	'use myApp\n db.test.insert({ x: 100501 })' \
	"myApp:myPass@${cluster}-mongos.${namespace}"
sleep 5
compare_mongos_cmd "find" "myApp:myPass@${cluster}-mongos.${namespace}" "-sharded"

# wait for stable timestamp in wiredtiger
log 'waiting 60 seconds for stable timestamp in wiredtiger'
sleep 80

log 'running backups'
if [ -z "$SKIP_BACKUPS_TO_AWS_GCP_AZURE" ]; then
	backup_name_aws="backup-aws-s3-sharded"
	backup_name_gcp="backup-gcp-cs-sharded"
	backup_name_azure="backup-azure-blob-sharded"

	run_backup aws-s3 ${backup_name_aws}
	run_backup gcp-cs ${backup_name_gcp}
	run_backup azure-blob ${backup_name_azure}

	wait_backup "${backup_name_aws}"
	check_backup_in_storage ${backup_name_aws} s3 rs0
	check_backup_in_storage ${backup_name_aws} s3 cfg

	wait_backup "${backup_name_gcp}"
	check_backup_in_storage ${backup_name_gcp} gcs rs0
	check_backup_in_storage ${backup_name_gcp} gcs cfg

	wait_backup "${backup_name_azure}"
	check_backup_in_storage ${backup_name_azure} azure rs0
	check_backup_in_storage ${backup_name_azure} azure cfg
fi

backup_name_minio="backup-minio-sharded"
run_backup minio ${backup_name_minio}
wait_backup "${backup_name_minio}"
check_backup_in_storage ${backup_name_minio} minio rs0

run_mongos \
	'use myApp\n db.test.insert({ x: 100502 })' \
	"myApp:myPass@${cluster}-mongos.${namespace}"
sleep 5
compare_mongos_cmd "find" "myApp:myPass@${cluster}-mongos.${namespace}" "-not-base-sharded"

backup_name_minio_not_base="backup-minio-not-base-sharded"
run_backup minio ${backup_name_minio_not_base} false
wait_backup "${backup_name_minio_not_base}"
check_backup_in_storage ${backup_name_minio_not_base} minio rs0

if [ -z "$SKIP_BACKUPS_TO_AWS_GCP_AZURE" ]; then
	run_restore "${backup_name_aws}" "_restore_sharded"
	run_recovery_check "${backup_name_aws}" "_restore_sharded"
	check_exported_mongos_service_endpoint "$lbEndpoint"

	run_restore "${backup_name_gcp}" "_restore_sharded"
	run_recovery_check "${backup_name_gcp}" "_restore_sharded"
	check_exported_mongos_service_endpoint "$lbEndpoint"

	run_restore "${backup_name_azure}" "_restore_sharded"
	run_recovery_check "${backup_name_azure}" "_restore_sharded"
	check_exported_mongos_service_endpoint "$lbEndpoint"
fi

run_restore ${backup_name_minio_not_base} "_restore_sharded"
run_recovery_check ${backup_name_minio_not_base} "_restore_sharded" false

run_restore ${backup_name_minio} "_restore_sharded"
run_recovery_check ${backup_name_minio} "_restore_sharded"

destroy "$namespace"
