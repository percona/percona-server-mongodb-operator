#!/bin/bash

set -o errexit

test_dir=$(realpath "$(dirname "$0")")
. "${test_dir}/../functions"
set_debug

FULL_VER=$(get_mongod_ver_from_image ${IMAGE_MONGOD})
MONGO_VER=${FULL_VER:0:3}
unset OPERATOR_NS

main_cluster="cross-site-sharded-main"
replica_cluster="cross-site-sharded-replica"

wait_for_members() {
  local endpoint="$1"
  local rsName="$2"
  local nodes_amount=0
  until [[ ${nodes_amount} == 6 ]]; do
    nodes_amount=$(run_mongos 'rs.conf().members.length' "clusterAdmin:clusterAdmin123456@$endpoint" "mongodb" ":27017" \
     | egrep -v 'I NETWORK|W NETWORK|Error saving history file|Percona Server for MongoDB|connecting to:|Unable to reach primary for set|Implicit session:|versions do not match|Error saving history file:|bye' \
     | $sed -re 's/ObjectId\("[0-9a-f]+"\)//; s/-[0-9]+.svc/-xxx.svc/')

    echo "waiting for all members to be configured in ${rsName}"
    let retry+=1
    if [ $retry -ge 15 ]; then
      echo "Max retry count $retry reached. something went wrong with mongo cluster. Config for endpoint $endpoint has $nodes_amount but expected 6."
      exit 1
    fi
    echo -n .
    sleep 10
  done
}

desc "create main cluster"
create_infra "$namespace"

desc 'create secrets and start client'
kubectl_bin apply \
	-f "$conf_dir/client.yml" \
	-f "$test_dir/conf/secrets.yml"

desc "create main PSMDB cluster $main_cluster."
apply_cluster "$test_dir/conf/$main_cluster.yml"

desc 'check if all 3 Pods started'
wait_for_running $main_cluster-rs0 3
wait_for_running $main_cluster-cfg 3 "false"

desc 'create user'
run_mongos \
	'db.createUser({user:"user",pwd:"pass",roles:[{db:"app",role:"readWrite"}]})' \
	"userAdmin:userAdmin123456@$main_cluster-mongos.$namespace"
sleep 2

desc 'set chunk size to 2 MB'
run_mongos \
	"use config\n db.settings.save( { _id:\"chunksize\", value: 2 } )" \
	"clusterAdmin:clusterAdmin123456@$main_cluster-mongos.$namespace"
sleep 2

desc 'write data'
run_script_mongos "${test_dir}/data.js" "user:pass@$main_cluster-mongos.$namespace"

desc 'shard collection'
run_mongos \
	'sh.enableSharding("app")' \
	"clusterAdmin:clusterAdmin123456@$main_cluster-mongos.$namespace"
sleep 2

run_mongos \
	'sh.shardCollection("app.city", { _id: 1 } )' \
	"clusterAdmin:clusterAdmin123456@$main_cluster-mongos.$namespace"
sleep 120

desc 'Check chunks'
chunks_param1="ns"
chunks_param2='"app.city"'

if [[ ${MONGO_VER} != "4.4" ]]; then
	chunks_param1="uuid"
	chunks_param2=$(run_mongos \
		"use app\n db.getCollectionInfos({ \"name\": \"city\" })[0].info.uuid" \
		"user:pass@$main_cluster-mongos.$namespace" \
		| grep "switched to db app" -A 1 | grep -v "switched to db app")
fi

shards=0
for i in "rs0" "rs1"; do
	out=$(run_mongos \
		"use config\n db.chunks.count({\"${chunks_param1}\": ${chunks_param2}, \"shard\": \"$i\"})" \
		"clusterAdmin:clusterAdmin123456@$main_cluster-mongos.$namespace" \
		| grep "switched to db config" -A 1 | grep -v "switched to db config")

	desc "$i has $out chunks"

	if [[ $out -ne 0 ]]; then
		((shards = shards + 1))
	fi
done

if [[ $shards -lt 2 ]]; then
	echo "data is only on some of the shards, maybe sharding is not working"
	exit 1
fi

desc "create replica cluster"
create_namespace $replica_namespace 0
deploy_operator

desc 'start client'
kubectl_bin apply \
	-f "$conf_dir/client.yml"

desc "copy secrets from main to replica namespace and create all of them"
kubectl get secret ${main_cluster}-secrets -o yaml -n ${namespace} \
	| yq eval '
		del(.metadata) |
		(.metadata.name = "'${replica_cluster}'-secrets")' - \
	| kubectl_bin apply -f -

kubectl_bin get secret ${main_cluster}-ssl-internal -o yaml -n ${namespace} \
	| yq eval '
		del(.metadata) |
		del(.status) |
		(.metadata.name = "'${replica_cluster}'-ssl-internal")' - \
	| kubectl_bin apply -f -

kubectl_bin get secret ${main_cluster}-ssl -o yaml -n ${namespace} \
	| yq eval '
		del(.metadata) |
		del(.status) |
		(.metadata.name = "'${replica_cluster}'-ssl")' - \
	| kubectl_bin apply -f -

sleep 30

desc "create replica PSMDB cluster $cluster"
apply_cluster "$test_dir/conf/${replica_cluster}.yml"

wait_for_running $replica_cluster-rs0 3 "false"
wait_for_running $replica_cluster-rs1 3 "false"
wait_for_running $replica_cluster-cfg 3 "false"

replica_cfg_0_endpoint=$(get_service_ip cross-site-sharded-replica-cfg-0 'cfg')
replica_cfg_1_endpoint=$(get_service_ip cross-site-sharded-replica-cfg-1 'cfg')
replica_cfg_2_endpoint=$(get_service_ip cross-site-sharded-replica-cfg-2 'cfg')
replica_rs0_0_endpoint=$(get_service_ip cross-site-sharded-replica-rs0-0)
replica_rs0_1_endpoint=$(get_service_ip cross-site-sharded-replica-rs0-1)
replica_rs0_2_endpoint=$(get_service_ip cross-site-sharded-replica-rs0-2)
replica_rs1_0_endpoint=$(get_service_ip cross-site-sharded-replica-rs1-0 'rs1')
replica_rs1_1_endpoint=$(get_service_ip cross-site-sharded-replica-rs1-1 'rs1')
replica_rs1_2_endpoint=$(get_service_ip cross-site-sharded-replica-rs1-2 'rs1')

kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"

kubectl_bin patch psmdb ${main_cluster} --type=merge --patch '{
			"spec": {"replsets":[
			  {"affinity":{"antiAffinityTopologyKey": "none"},"arbiter":{"affinity":{"antiAffinityTopologyKey": "none"},"enabled":false,"size":1},"expose":{"enabled":true,"exposeType":"ClusterIp"},"externalNodes":[{"host":"'${replica_rs0_0_endpoint}'","priority":0,"votes":0},{"host":"'${replica_rs0_1_endpoint}'","port":27017,"priority":1,"votes":1},{"host":"'${replica_rs0_2_endpoint}'", "port":27017,"priority":1,"votes":1}],"name":"rs0","nonvoting":{"affinity":{"antiAffinityTopologyKey":"none"},"enabled":false,"podDisruptionBudget":{"maxUnavailable":1},"resources":{"limits":{"cpu":"300m","memory":"0.5G"},"requests":{"cpu":"300m","memory":"0.5G"}},"size":3,"volumeSpec":{"persistentVolumeClaim":{"resources":{"requests":{"storage":"1Gi"}}}}},"podDisruptionBudget":{"maxUnavailable":1},"resources":{"limits":{"cpu":"300m","memory":"0.5G"},"requests":{"cpu":"300m","memory":"0.5G"}},"size":3,"volumeSpec":{"persistentVolumeClaim":{"resources":{"requests":{"storage":"3Gi"}}}}},
			  {"affinity":{"antiAffinityTopologyKey": "none"},"arbiter":{"affinity":{"antiAffinityTopologyKey": "none"},"enabled":false,"size":1},"expose":{"enabled":true,"exposeType":"ClusterIp"},"externalNodes":[{"host":"'${replica_rs1_0_endpoint}'","priority":0,"votes":0},{"host":"'${replica_rs1_1_endpoint}'","port":27017,"priority":1,"votes":1},{"host":"'${replica_rs1_2_endpoint}'", "port":27017,"priority":1,"votes":1}],"name":"rs1","nonvoting":{"affinity":{"antiAffinityTopologyKey":"none"},"enabled":false,"podDisruptionBudget":{"maxUnavailable":1},"resources":{"limits":{"cpu":"300m","memory":"0.5G"},"requests":{"cpu":"300m","memory":"0.5G"}},"size":3,"volumeSpec":{"persistentVolumeClaim":{"resources":{"requests":{"storage":"1Gi"}}}}},"podDisruptionBudget":{"maxUnavailable":1},"resources":{"limits":{"cpu":"300m","memory":"0.5G"},"requests":{"cpu":"300m","memory":"0.5G"}},"size":3,"volumeSpec":{"persistentVolumeClaim":{"resources":{"requests":{"storage":"3Gi"}}}}}
			  ],
			  "sharding":{"configsvrReplSet":{ "externalNodes": [{"host":"'${replica_cfg_0_endpoint}'","priority":1,"votes":1 },{"host":"'${replica_cfg_1_endpoint}'", "priority":1,"votes":1},{"host":"'${replica_cfg_2_endpoint}'","priority":0,"votes":0}]}}
			  }
		}'

wait_for_members $replica_cfg_0_endpoint cfg
wait_for_members $replica_rs0_0_endpoint rs0
wait_for_members $replica_rs1_0_endpoint rs1

kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$replica_namespace"

desc 'check if all 3 Pods started'
wait_for_running $replica_cluster-rs0 3
wait_for_running $replica_cluster-cfg 3 "false"

desc 'create user'
run_mongos \
	'db.createUser({user:"myApp",pwd:"myPass",roles:[{db:"myApp",role:"readWrite"}]})' \
	"userAdmin:userAdmin123456@$main_cluster-mongos.$namespace"
sleep 2

desc 'write data, read from all'
run_mongos \
	'use myApp\n db.test.insert({ x: 100500 })' \
	"myApp:myPass@$main_cluster-mongos.$namespace"

minikube_sleep
desc "Compare data"
compare_mongos_cmd "find" "myApp:myPass@$main_cluster-mongos.$namespace"

desc 'test failover'
kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"

kubectl_bin delete psmdb $main_cluster

desc 'run disaster recovery script for replset: cfg'
run_script_mongos "${test_dir}/disaster_recovery.js" "clusterAdmin:clusterAdmin123456@$replica_cfg_0_endpoint" "mongodb" ":27017"

desc 'run disaster recovery script for replset: rs0'
run_script_mongos "${test_dir}/disaster_recovery.js" "clusterAdmin:clusterAdmin123456@$replica_rs0_0_endpoint" "mongodb" ":27017"

desc 'run disaster recovery script for replset: rs1'
run_script_mongos "${test_dir}/disaster_recovery.js" "clusterAdmin:clusterAdmin123456@$replica_rs1_0_endpoint" "mongodb" ":27017"

desc 'make replica cluster managed'
kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$replica_namespace"
kubectl_bin patch psmdb ${replica_cluster} --type=merge --patch '{"spec":{"unmanaged": false}}'

wait_for_running $replica_cluster-rs0 3
wait_for_running $replica_cluster-cfg 3

desc "check failover status"
compare_mongos_cmd "find" "myApp:myPass@$replica_cluster-mongos.$replica_namespace"
desc "Failover check finished successfully"

wait_cluster_consistency ${replica_cluster}

destroy "$namespace" "true"

destroy $replica_namespace "true"

desc 'test passed'
