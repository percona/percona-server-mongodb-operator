#!/bin/bash

set -o errexit

test_dir=$(realpath $(dirname $0))
. ${test_dir}/../functions
set_debug
# EKS cluster should be run without the policy AmazonS3FullAccess
# This policy makes the test false passed.

if [ $EKS -ne 1 ]; then
	echo "Skip the test. We run it for EKS only "
	exit 0
fi
cluster="some-name-rs0"

# get cluster oidc
eks_cluster=$(kubectl config view --minify -o jsonpath='{.contexts[0].context.cluster}' | awk -F/ '{print $NF}')
IFS='.' read -r eks_cluster_name eks_cluster_region _ <<< "$eks_cluster"

eks_cluster_oidc=$(aws eks describe-cluster --name $eks_cluster_name --region=$eks_cluster_region  --query "cluster.identity.oidc.issuer" --output text | sed 's|https://||')
policy_arn="arn:aws:iam::119175775298:policy/operator-testing-access-s3"
role_name="$cluster-psmdb-access-s3-bucket"

# Create policy. Already done, we don't need to do it every time. But all steps should be illustrated in the
#aws iam create-policy --policy-name operator-testing-allow-access-s3 --policy-document file://conf/s3-bucket-policy.json


desc "create role"
jq --arg eks_cluster_oidc "$eks_cluster_oidc" \
   '.Statement[0].Principal.Federated = "arn:aws:iam::119175775298:oidc-provider/\($eks_cluster_oidc)" |
    .Statement[0].Condition.StringEquals["$eks_cluster_oidc:aud"] = "sts.amazonaws.com"' \
   $test_dir/conf/template.json > $test_dir/conf/role-trust-policy.json

role_arn=$(aws iam create-role \
    --role-name "$role_name" \
    --assume-role-policy-document file://$test_dir/conf/role-trust-policy.json \
    --description "Allow access to s3 bucket" \
    --query "Role.Arn" \
    --output text || true)

role_arn="arn:aws:iam::119175775298:role/some-name-rs0-psmdb-access-s3-bucket"

desc "connect role and policy"
aws iam attach-role-policy --role-name "$role_name" --policy-arn $policy_arn

create_infra "$namespace"

desc "create secrets and start client"
kubectl_bin apply \
	-f "$conf_dir/secrets.yml" \
	-f "$conf_dir/client.yml"

desc "create first PSMDB cluster $cluster"
apply_cluster $test_dir/conf/$cluster.yml

desc 'check if all 3 Pods started'
wait_for_running $cluster 3

desc 'check if service and statefulset created with expected config'
compare_kubectl statefulset/$cluster

desc "update service accounts for operator and default (our cluster uses this one)"

kubectl_bin annotate serviceaccount default \
    eks.amazonaws.com/role-arn="$role_arn" \
    --overwrite

kubectl_bin annotate serviceaccount percona-server-mongodb-operator \
    eks.amazonaws.com/role-arn="$role_arn" \
    --overwrite

desc "restart operator and cluster"
operator_pod=$(get_operator_pod)
kubectl_bin delete pod $operator_pod

kubectl_bin delete pod "$cluster-0"
kubectl_bin delete pod "$cluster-1"
kubectl_bin delete pod "$cluster-2"

wait_for_running $cluster 3

kubectl exec $cluster-0 -c backup-agent -- sh -c 'if [ -z "$AWS_ROLE_ARN" ]; then echo "Variable AWS_ROLE_ARN not set" && exit 1; else echo "Variable AWS_ROLE_ARN is set"; fi'

desc 'create user'
run_mongo \
	'db.createUser({user:"myApp",pwd:"myPass",roles:[{db:"myApp",role:"readWrite"}]})' \
	"userAdmin:userAdmin123456@$cluster.$namespace"
sleep 2

desc 'write data, read from all'
run_mongo \
	'use myApp\n db.test.insert({ x: 100500 })' \
	"myApp:myPass@$cluster.$namespace"

minikube_sleep

desc "compare mongo cmd"
compare_mongo_cmd "find" "myApp:myPass@$cluster-0.$cluster.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-1.$cluster.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-2.$cluster.$namespace"


#desc "wait backup agent"
wait_backup_agent $cluster-0
wait_backup_agent $cluster-1
wait_backup_agent $cluster-2

backup_name_aws="backup-aws-s3"

desc 'run backups'
run_backup aws-s3
wait_backup "$backup_name_aws"
sleep 5

desc 'check backup and restore -- aws-s3'
backup_dest_aws=$(get_backup_dest "$backup_name_aws")
curl -s "https://s3.amazonaws.com/${backup_dest_aws}/rs0/myApp.test.gz" | gunzip >/dev/null
run_mongo 'use myApp\n db.test.insert({ x: 100501 })' "myApp:myPass@$cluster.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-0.$cluster.$namespace" "-2nd"
compare_mongo_cmd "find" "myApp:myPass@$cluster-1.$cluster.$namespace" "-2nd"
compare_mongo_cmd "find" "myApp:myPass@$cluster-2.$cluster.$namespace" "-2nd"
run_restore "$backup_name_aws"
wait_restore "$backup_name_aws" "${cluster/-rs0/}"
compare_mongo_cmd "find" "myApp:myPass@$cluster-0.$cluster.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-1.$cluster.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-2.$cluster.$namespace"

desc 'delete backup and check if it is removed from bucket -- aws-s3'
kubectl_bin delete psmdb-backup --all
check_backup_deletion "https://s3.amazonaws.com/${backup_dest_aws}" "aws-s3"

destroy $namespace

desc "delete role"
aws iam detach-role-policy --role-name "$role_name" --policy-arn "$policy_arn"
aws iam delete-role --role-name "$role_name"

desc 'test passed'
