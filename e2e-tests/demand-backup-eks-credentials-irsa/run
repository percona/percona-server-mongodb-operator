#!/bin/bash

set -o errexit

test_dir=$(realpath $(dirname $0))
. ${test_dir}/../functions
set_debug
# EKS cluster should be run without the policy AmazonS3FullAccess
# This policy makes the test false passed.

if [ $EKS -ne 1 ]; then
	echo "Skip the test. We run it for EKS only "
	exit 0
fi

cluster="some-name"

desc "get cluster oidc"
eks_cluster=$(kubectl config view --minify -o jsonpath='{.contexts[0].context.cluster}' | awk -F/ '{print $NF}')
IFS='.' read -r eks_cluster_name eks_cluster_region _ <<<"$eks_cluster"

eks_cluster_oidc=$(aws eks describe-cluster --name $eks_cluster_name --region=$eks_cluster_region --query "cluster.identity.oidc.issuer" --output text | sed 's|https://||')
policy_arn="arn:aws:iam::119175775298:policy/operator-testing-access-s3"
role_name="$cluster-psmdb-access-s3-bucket"

desc "delete role"
echo $role_name
echo $policy_arn
aws iam detach-role-policy --role-name "$role_name" --policy-arn "$policy_arn" || true
aws iam delete-role --role-name "$role_name" || true

# Create policy. Already done, we don't need to do it every time. But all steps should be illustrated in the test
#aws iam create-policy --policy-name operator-testing-allow-access-s3 --policy-document file://conf/s3-bucket-policy.json

desc "create role"
jq --arg eks_cluster_oidc "$eks_cluster_oidc" \
	'.Statement[0].Principal.Federated = "arn:aws:iam::119175775298:oidc-provider/\($eks_cluster_oidc)" |
    .Statement[0].Condition.StringEquals[($eks_cluster_oidc + ":aud")] = "sts.amazonaws.com"' \
	$test_dir/conf/template.json >$test_dir/conf/role-trust-policy.json

role_arn=$(aws iam create-role \
	--role-name "$role_name" \
	--assume-role-policy-document file://$test_dir/conf/role-trust-policy.json \
	--description "Allow access to s3 bucket" \
	--query "Role.Arn" \
	--output text)

desc "connect role and policy"
aws iam attach-role-policy --role-name "$role_name" --policy-arn $policy_arn

create_infra "$namespace"

desc "create secrets and start client"
kubectl_bin apply \
	-f "$conf_dir/secrets.yml" \
	-f "$conf_dir/client.yml"

desc "create PSMDB cluster $cluster"
apply_cluster $test_dir/conf/$cluster.yml

desc 'check if all 3 Pods started'
wait_for_running $cluster-rs0 3

desc 'check if service and statefulset created with expected config'
compare_kubectl statefulset/$cluster-rs0

desc "update service accounts for operator and default (our cluster uses this one)"

kubectl_bin annotate serviceaccount default \
	eks.amazonaws.com/role-arn="$role_arn" \
	--overwrite

kubectl_bin annotate serviceaccount percona-server-mongodb-operator \
	eks.amazonaws.com/role-arn="$role_arn" \
	--overwrite

desc "restart operator and cluster"
operator_pod=$(get_operator_pod)
kubectl_bin delete pod $operator_pod

kubectl_bin delete pod "$cluster-rs0-0"
kubectl_bin delete pod "$cluster-rs0-1"
kubectl_bin delete pod "$cluster-rs0-2"

wait_for_running $cluster-rs0 3

kubectl exec $cluster-rs0-0 -c backup-agent -- sh -c 'if [ -z "$AWS_ROLE_ARN" ]; then echo "Variable AWS_ROLE_ARN not set" && exit 1; else echo "Variable AWS_ROLE_ARN is set"; fi'

desc 'create user'
run_mongo \
	'db.createUser({user:"myApp",pwd:"myPass",roles:[{db:"myApp",role:"readWrite"}]})' \
	"userAdmin:userAdmin123456@$cluster-rs0.$namespace"
sleep 2

desc 'write data, read from all'
run_mongo \
	'use myApp\n db.test.insert({ x: 100500 })' \
	"myApp:myPass@$cluster-rs0.$namespace"

desc "compare mongo cmd"
compare_mongo_cmd "find" "myApp:myPass@$cluster-rs0-0.$cluster-rs0.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-rs0-1.$cluster-rs0.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-rs0-2.$cluster-rs0.$namespace"

desc "wait backup agent"
wait_backup_agent $cluster-rs0-0
wait_backup_agent $cluster-rs0-1
wait_backup_agent $cluster-rs0-2

backup_name_aws="backup-aws-s3"

desc 'run backups'
run_backup aws-s3
wait_backup "$backup_name_aws"
sleep 5

desc 'check backup and restore -- aws-s3'
backup_dest_aws=$(get_backup_dest "$backup_name_aws")
curl -s "https://s3.amazonaws.com/${backup_dest_aws}/rs0/myApp.test.gz" | gunzip >/dev/null
run_mongo 'use myApp\n db.test.insert({ x: 100501 })' "myApp:myPass@$cluster-rs0.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-rs0-0.$cluster-rs0.$namespace" "-2nd"
compare_mongo_cmd "find" "myApp:myPass@$cluster-rs0-1.$cluster-rs0.$namespace" "-2nd"
compare_mongo_cmd "find" "myApp:myPass@$cluster-rs0-2.$cluster-rs0.$namespace" "-2nd"

run_restore "$backup_name_aws"

wait_restore "$backup_name_aws" "${cluster}"
compare_mongo_cmd "find" "myApp:myPass@$cluster-rs0-0.$cluster-rs0.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-rs0-1.$cluster-rs0.$namespace"
compare_mongo_cmd "find" "myApp:myPass@$cluster-rs0-2.$cluster-rs0.$namespace"
desc 'delete backup and check if it is removed from bucket -- aws-s3'
kubectl_bin delete psmdb-backup --all

desc 'check pitr -- aws-s3'

backup_name_aws="backup-aws-s3-pitr"
backup_dest_aws=$(get_backup_dest "$backup_name_aws")
run_backup aws-s3 "${backup_name_aws}" "logical"
desc "wait backup"
wait_backup "$backup_name_aws"
sleep 5

run_mongo 'use myApp\n db.test.insert({ x: 100502 })' "myApp:myPass@$cluster-rs0.$namespace"
desc 'compare'
compare_mongo_cmd "find" "myApp:myPass@${cluster}-rs0-0.$cluster-rs0.$namespace" "-3nd"
compare_mongo_cmd "find" "myApp:myPass@${cluster}-rs0-1.$cluster-rs0.$namespace" "-3nd"
compare_mongo_cmd "find" "myApp:myPass@${cluster}-rs0-2.$cluster-rs0.$namespace" "-3nd"

run_pitr_check "${backup_name_aws}" "${cluster}" "-3nd"

destroy $namespace

desc 'test passed'
